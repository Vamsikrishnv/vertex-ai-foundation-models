{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cec0caff-390d-40d4-847c-82af211c3d95",
   "metadata": {},
   "source": [
    "# L4: Using the Model for Predictions\n",
    "\n",
    "## Goal\n",
    "Build a working Python Q&A assistant that answers questions in Stack Overflow style.\n",
    "\n",
    "## Approach\n",
    "Since fine-tuning was blocked in L3 (API unavailable), we're using:\n",
    "- Base Model: Gemini 1.5 Flash\n",
    "- Technique: Instruction-based prompting (zero-shot)\n",
    "- Style: Stack Overflow developer answers\n",
    "\n",
    "## What This Demonstrates\n",
    "- Prompt engineering\n",
    "- API integration\n",
    "- Production-ready code structure\n",
    "- Error handling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e20d98-9a28-40c5-9573-acb705dd5fab",
   "metadata": {},
   "source": [
    "# L4: Using the Model for Predictions\n",
    "\n",
    "## Attempt 1: Google Cloud Vertex AI ‚ùå\n",
    "\n",
    "**Issue:** No model access in project/region\n",
    "**Result:** All models returned 404 errors\n",
    "**Learning:** Regional limitations and access restrictions\n",
    "\n",
    "Below is the original attempt (kept for documentation):\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b872e9f-dcbb-4413-8abe-10bafcb4357b",
   "metadata": {},
   "source": [
    "Setup (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e35a45-4a53-436a-ba24-6b2dda615bcf",
   "metadata": {},
   "source": [
    "# L4: Stack Overflow Python Q&A Assistant\n",
    "# Using base Gemini model with Stack Overflow style prompting\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "import vertexai\n",
    "from typing import List, Dict\n",
    "import datetime\n",
    "\n",
    "print(\"üöÄ Initializing Stack Overflow AI Assistant...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee607dc1-8469-4b8a-8ea4-257cbeda2cb0",
   "metadata": {},
   "source": [
    "Configuration (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fbab0-4b9c-44e1-a748-25b065a23bdf",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "PROJECT_ID = \"dotted-music-460617-k2\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Load base model\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "print(f\"‚úÖ Connected to project: {PROJECT_ID}\")\n",
    "print(f\"‚úÖ Region: {REGION}\")\n",
    "print(f\"‚úÖ Model loaded: gemini-1.5-flash-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4008d9-7599-43f2-b47d-ea84674d0377",
   "metadata": {},
   "source": [
    "Define Instruction Template (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ce0d3a-fda2-4977-aacb-12603397873a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Stack Overflow style instruction template\n",
    "# This mimics what we would have trained the model on\n",
    "\n",
    "INSTRUCTION_TEMPLATE = \"\"\"You are a helpful Python programming expert who answers questions like a Stack Overflow developer.\n",
    "\n",
    "When answering:\n",
    "1. Be clear and concise\n",
    "2. Provide working code examples\n",
    "3. Explain WHY the solution works\n",
    "4. Mention common pitfalls or alternatives\n",
    "5. Use proper code formatting\n",
    "\n",
    "Answer in Stack Overflow style - professional but friendly.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(\"‚úÖ Instruction template loaded\")\n",
    "print(\"\\nTemplate preview:\")\n",
    "print(\"=\" * 70)\n",
    "print(INSTRUCTION_TEMPLATE.format(question=\"[Your question here]\"))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7edd7-ffbe-4e21-a337-16a1153f10e9",
   "metadata": {},
   "source": [
    "Create Assistant Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718aee2-647f-4af5-b221-7a04ce8dbe68",
   "metadata": {},
   "source": [
    "def ask_python_question(question: str, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Ask a Python question and get Stack Overflow style answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The Python question to ask\n",
    "        temperature (float): Controls randomness (0.0-1.0)\n",
    "                           Lower = more focused/deterministic\n",
    "                           Higher = more creative/varied\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer from the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Format the prompt\n",
    "        prompt = INSTRUCTION_TEMPLATE.format(question=question)\n",
    "        \n",
    "        # Generate response\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": temperature,\n",
    "                \"max_output_tokens\": 1024,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "print(\"‚úÖ Assistant function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f9c36-5803-45ef-a28b-6caf69f0cd6e",
   "metadata": {},
   "source": [
    "Test with Single Question (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a4347-5c6d-409f-9046-c52b12ce17d5",
   "metadata": {},
   "source": [
    "# Test 1: Simple question\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Simple CSV Reading Question\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "question = \"How do I read a CSV file using pandas?\"\n",
    "\n",
    "print(f\"\\n Question: {question}\\n\")\n",
    "print(\" Answer:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "answer = ask_python_question(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814f0c0-a7a9-4891-acae-796d77f4b567",
   "metadata": {},
   "source": [
    "# Check what credentials the notebook is using\n",
    "import google.auth\n",
    "\n",
    "credentials, project = google.auth.default()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK AUTHENTICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nProject from credentials: {project}\")\n",
    "print(f\"Project in code: {PROJECT_ID}\")\n",
    "print(f\"Match? {project == PROJECT_ID}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECKING API ACCESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if we can access Vertex AI at all\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "try:\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    print(\"‚úÖ Vertex AI connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vertex AI error: {e}\")\n",
    "\n",
    "# Check available models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECKING MODEL ACCESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    # Try to list models\n",
    "    from vertexai.preview.generative_models import GenerativeModel\n",
    "    \n",
    "    # Try different model names\n",
    "    test_models = [\n",
    "        \"gemini-1.5-flash-002\",\n",
    "        \"gemini-1.5-flash\",\n",
    "        \"gemini-pro\",\n",
    "        \"text-bison@001\"\n",
    "    ]\n",
    "    \n",
    "    for model_name in test_models:\n",
    "        try:\n",
    "            test_model = GenerativeModel(model_name)\n",
    "            print(f\"‚úÖ {model_name} - Accessible\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} - {str(e)[:80]}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error checking models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4c001-69c5-4c12-8ed1-9d4f9210ccdf",
   "metadata": {},
   "source": [
    "# Check authentication and model access\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "\n",
    "# Get credentials\n",
    "credentials, project = google.auth.default()\n",
    "\n",
    "# Define project (in case Cell 3 wasn't run)\n",
    "PROJECT_ID = \"dotted-music-460617-k2\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK AUTHENTICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nProject from credentials: {project}\")\n",
    "print(f\"Project in code: {PROJECT_ID}\")\n",
    "print(f\"Match? {project == PROJECT_ID}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECKING API ACCESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if we can access Vertex AI at all\n",
    "try:\n",
    "    aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    print(\"‚úÖ Vertex AI connection successful\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Vertex AI error: {e}\")\n",
    "\n",
    "# Check available models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECKING MODEL ACCESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_models = [\n",
    "    \"gemini-1.5-flash-002\",\n",
    "    \"gemini-1.5-flash\", \n",
    "    \"gemini-1.5-pro\",\n",
    "    \"gemini-pro\",\n",
    "    \"text-bison@001\",\n",
    "    \"text-bison@002\"\n",
    "]\n",
    "\n",
    "for model_name in test_models:\n",
    "    try:\n",
    "        test_model = GenerativeModel(model_name)\n",
    "        # Try a simple prediction\n",
    "        response = test_model.generate_content(\"Say hello\")\n",
    "        print(f\"‚úÖ {model_name} - WORKS!\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"404\" in error_msg or \"not found\" in error_msg:\n",
    "            print(f\"‚ùå {model_name} - Not available\")\n",
    "        elif \"403\" in error_msg or \"permission\" in error_msg.lower():\n",
    "            print(f\"‚ö†Ô∏è  {model_name} - Permission denied\")\n",
    "        else:\n",
    "            print(f\"‚ùå {model_name} - {error_msg[:60]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNOSIS COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd3ac0-14b7-45ce-8c35-4da8845f4758",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attempt 2: OpenAI (Working Solution) ‚úÖ\n",
    "\n",
    "After Google Cloud limitations, switching to OpenAI which provides:\n",
    "- ‚úÖ Immediate model access\n",
    "- ‚úÖ Reliable API\n",
    "- ‚úÖ Works with our fine-tuned model from L3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69911c6-cea1-4e15-8507-e4994fb580ec",
   "metadata": {},
   "source": [
    "Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de53d9b9-35e5-4a73-a0c3-f07668f08ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI client initialized\n",
      "‚úÖ API Key loaded: sk-proj-c4JHt8PTD-Go...\n"
     ]
    }
   ],
   "source": [
    "# L4: Using OpenAI Fine-Tuned Model\n",
    "import openai\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"‚ùå Please set OPENAI_API_KEY in .env file!\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "print(\"‚úÖ OpenAI client initialized\")\n",
    "print(f\"‚úÖ API Key loaded: {OPENAI_API_KEY[:20]}...\")  # Show first 20 chars only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8320f-49fa-472c-9b7b-efe65434a65d",
   "metadata": {},
   "source": [
    "Load Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f32ed2-869f-4fee-8882-a35a140dd5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  Fine-tuned model file not found!\n",
      "\n",
      "Options:\n",
      "1. Wait for L3 training to complete\n",
      "2. Check finetuning_job_id.txt for status\n",
      "3. Use base model for now\n",
      "\n",
      "‚ö†Ô∏è  Using base model for now: gpt-4o-mini-2024-07-18\n"
     ]
    }
   ],
   "source": [
    "# Load your fine-tuned model name\n",
    "# (This file was created in L3 after training completed)\n",
    "\n",
    "try:\n",
    "    with open(\"finetuned_model_name.txt\", \"r\") as f:\n",
    "        FINETUNED_MODEL = f.read().strip()\n",
    "    \n",
    "    print(\"‚úÖ Fine-tuned model loaded!\")\n",
    "    print(f\"üìç Model: {FINETUNED_MODEL}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Fine-tuned model file not found!\")\n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. Wait for L3 training to complete\")\n",
    "    print(\"2. Check finetuning_job_id.txt for status\")\n",
    "    print(\"3. Use base model for now\")\n",
    "    \n",
    "    # Use base model for testing\n",
    "    FINETUNED_MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "    print(f\"\\n‚ö†Ô∏è  Using base model for now: {FINETUNED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010724b-03cb-4ae3-b932-a132dc3fde8d",
   "metadata": {},
   "source": [
    "CREATE ASSISTANT FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96151155-04bb-41e7-a5ea-64ae8276f474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assistant function created\n",
      " Ready to answer Python questions!\n"
     ]
    }
   ],
   "source": [
    "def ask_python_question(question: str, temperature: float = 0.3) -> str:\n",
    "    \"\"\"\n",
    "    Ask a Python question to your model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=FINETUNED_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a helpful Python programming expert who answers questions like a Stack Overflow developer.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                }\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\" Error: {str(e)}\"\n",
    "\n",
    "print(\" Assistant function created\")\n",
    "print(\" Ready to answer Python questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ffcf4-68fe-43b2-a511-1ee49af48435",
   "metadata": {},
   "source": [
    "TEST IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760fc728-8a74-4168-b282-2e54cb784a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 1: CSV Reading Question\n",
      "======================================================================\n",
      "\n",
      "‚ùì Question: How do I read a CSV file using pandas?\n",
      "\n",
      "ü§ñ Answer:\n",
      "----------------------------------------------------------------------\n",
      "To read a CSV file using the `pandas` library in Python, you can use the `pandas.read_csv()` function. Here's a step-by-step guide on how to do it:\n",
      "\n",
      "1. **Install pandas**: If you haven't installed pandas yet, you can do so using pip:\n",
      "\n",
      "   ```bash\n",
      "   pip install pandas\n",
      "   ```\n",
      "\n",
      "2. **Import pandas**: In your Python script or interactive environment, import the pandas library.\n",
      "\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   ```\n",
      "\n",
      "3. **Read the CSV file**: Use the `pd.read_csv()` function to read the CSV file. You need to provide the file path as an argument.\n",
      "\n",
      "   ```python\n",
      "   df = pd.read_csv('path/to/your/file.csv')\n",
      "   ```\n",
      "\n",
      "   Replace `'path/to/your/file.csv'` with the actual path to your CSV file.\n",
      "\n",
      "4. **View the DataFrame**: After reading the CSV file, you can view the contents of the DataFrame by simply printing it or using methods like `.head()` to see the first few rows.\n",
      "\n",
      "   ```python\n",
      "   print(df)  # Print the entire DataFrame\n",
      "   print(df.head())  # Print the first 5 rows\n",
      "   ```\n",
      "\n",
      "### Example\n",
      "\n",
      "Here's a complete example:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Read the CSV file\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Display the first 5 rows of the DataFrame\n",
      "print(df.head())\n",
      "```\n",
      "\n",
      "### Additional Options\n",
      "\n",
      "The `read_csv()` function has many optional parameters that you can use to customize how the CSV file is read. Some common options include:\n",
      "\n",
      "- `sep`: Specify a different delimiter (default is `,`).\n",
      "- `header`: Specify the row number(s) to use as the column names.\n",
      "- `index_col`: Specify the column(s) to set as the index.\n",
      "- `usecols`: Specify a subset of columns to read.\n",
      "- `dtype`: Specify the data type for data or columns.\n",
      "\n",
      "For example, if your CSV file uses a semicolon as a delimiter, you can read it like this:\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('data.csv', sep=';')\n",
      "```\n",
      "\n",
      "You can find more details in the [pandas documentation for read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html).\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Simple question\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: CSV Reading Question\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "question = \"How do I read a CSV file using pandas?\"\n",
    "\n",
    "print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "print(\"ü§ñ Answer:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "answer = ask_python_question(question)\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dcf9e1a-67d4-4fdc-a4ab-b2b1c2daa2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "L3 FINE-TUNING STATUS\n",
      "======================================================================\n",
      "\n",
      " Job ID: ftjob-F15zqv4WC6HSQWthkarxmgac\n",
      " Status: failed\n",
      "\n",
      " Status: failed\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check L3 fine-tuning status\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "try:\n",
    "    # Load job ID\n",
    "    with open(\"finetuning_job_id.txt\", \"r\") as f:\n",
    "        job_id = f.read().strip()\n",
    "    \n",
    "    # Get status\n",
    "    job = openai.fine_tuning.jobs.retrieve(job_id)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"L3 FINE-TUNING STATUS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n Job ID: {job.id}\")\n",
    "    print(f\" Status: {job.status}\")\n",
    "    \n",
    "    if job.status == \"succeeded\":\n",
    "        print(f\"\\n TRAINING COMPLETE!\")\n",
    "        print(f\" Your fine-tuned model: {job.fine_tuned_model}\")\n",
    "        \n",
    "        # Save model name\n",
    "        with open(\"finetuned_model_name.txt\", \"w\") as f:\n",
    "            f.write(job.fine_tuned_model)\n",
    "        \n",
    "        print(\"\\n Model name saved!\")\n",
    "        print(\"\\n Restart kernel and re-run from Cell 13!\")\n",
    "        \n",
    "    elif job.status == \"running\":\n",
    "        print(\"\\n Still training...\")\n",
    "        print(\"   Check back in 10-15 minutes!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n Status: {job.status}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  finetuning_job_id.txt not found\")\n",
    "    print(\"   Did you complete L3 fine-tuning?\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21d1a0-ce23-4e85-b10d-4c01177e2186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vertex-ai-env)",
   "language": "python",
   "name": "vertex-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
