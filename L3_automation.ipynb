{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e27599-7270-464f-8e6d-c536e2991b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\projects\\vertex_ai_course\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ea35aa-d40c-4e4b-8071-61c97f233185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"kfp.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb03b9b-6a3f-4cba-96f0-0950cdc76378",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def say_hello(name: str) -> str:\n",
    "    hello_text = f\"Hello, {name}!\"\n",
    "    return hello_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b51b02d-423e-440e-8357-282c75b2ca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x0000023602197010>\n",
      "{{channel:task=say-hello;name=Output;type=String;}}\n"
     ]
    }
   ],
   "source": [
    "hello_task = say_hello(name=\"Erwin\")\n",
    "print(hello_task)\n",
    "print(hello_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a135168-0c6c-4508-bf08-3f64fd07992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def how_are_you(hello_text: str) -> str:\n",
    "    how_are_you = f\"{hello_text}. How are you?\"\n",
    "    return how_are_you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245f1d68-95fa-4d8a-9f5f-a329ce55788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<kfp.dsl.pipeline_task.PipelineTask object at 0x00000236021AAED0>\n",
      "{{channel:task=how-are-you;name=Output;type=String;}}\n"
     ]
    }
   ],
   "source": [
    "how_task = how_are_you(hello_text=hello_task.output)\n",
    "print(how_task)\n",
    "print(how_task.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56128723-a652-48a7-9406-ef15a0b0d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline\n",
    "def hello_pipeline(recipient: str) -> str:\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    how_task = how_are_you(hello_text=hello_task.output)\n",
    "    return how_task.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4ef072-76f0-4e02-9497-86f34fa81325",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(hello_pipeline, \"pipeline.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9e4c87-960e-4c5a-b48a-df09de8bd925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PIPELINE DEFINITION\n",
      "# Name: hello-pipeline\n",
      "# Inputs:\n",
      "#    recipient: str\n",
      "# Outputs:\n",
      "#    Output: str\n",
      "components:\n",
      "  comp-how-are-you:\n",
      "    executorLabel: exec-how-are-you\n",
      "    inputDefinitions:\n",
      "      parameters:\n",
      "        hello_text:\n",
      "          parameterType: STRING\n",
      "    outputDefinitions:\n",
      "      parameters:\n",
      "        Output:\n",
      "          parameterType: STRING\n",
      "  comp-say-hello:\n",
      "    executorLabel: exec-say-hello\n",
      "    inputDefinitions:\n",
      "      parameters:\n",
      "        name:\n",
      "          parameterType: STRING\n",
      "    outputDefinitions:\n",
      "      parameters:\n",
      "        Output:\n",
      "          parameterType: STRING\n",
      "deploymentSpec:\n",
      "  executors:\n",
      "    exec-how-are-you:\n",
      "      container:\n",
      "        args:\n",
      "        - --executor_input\n",
      "        - '{{$}}'\n",
      "        - --function_to_execute\n",
      "        - how_are_you\n",
      "        command:\n",
      "        - sh\n",
      "        - -c\n",
      "        - \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip ||\\\n",
      "          \\ python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1\\\n",
      "          \\ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\\\n",
      "          \\ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"\\\n",
      "          $0\\\" \\\"$@\\\"\\n\"\n",
      "        - sh\n",
      "        - -ec\n",
      "        - 'program_path=$(mktemp -d)\n",
      "\n",
      "\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n",
      "\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
      "\n",
      "          '\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n",
      "          \\ *\\n\\ndef how_are_you(hello_text: str) -> str:\\n    how_are_you = f\\\"{hello_text}.\\\n",
      "          \\ How are you?\\\"\\n    return how_are_you\\n\\n\"\n",
      "        image: python:3.7\n",
      "    exec-say-hello:\n",
      "      container:\n",
      "        args:\n",
      "        - --executor_input\n",
      "        - '{{$}}'\n",
      "        - --function_to_execute\n",
      "        - say_hello\n",
      "        command:\n",
      "        - sh\n",
      "        - -c\n",
      "        - \"\\nif ! [ -x \\\"$(command -v pip)\\\" ]; then\\n    python3 -m ensurepip ||\\\n",
      "          \\ python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1\\\n",
      "          \\ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.4.0'\\\n",
      "          \\ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\\\"3.9\\\"' && \\\"\\\n",
      "          $0\\\" \\\"$@\\\"\\n\"\n",
      "        - sh\n",
      "        - -ec\n",
      "        - 'program_path=$(mktemp -d)\n",
      "\n",
      "\n",
      "          printf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n",
      "\n",
      "          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
      "\n",
      "          '\n",
      "        - \"\\nimport kfp\\nfrom kfp import dsl\\nfrom kfp.dsl import *\\nfrom typing import\\\n",
      "          \\ *\\n\\ndef say_hello(name: str) -> str:\\n    hello_text = f\\\"Hello, {name}!\\\"\\\n",
      "          \\n    return hello_text\\n\\n\"\n",
      "        image: python:3.7\n",
      "pipelineInfo:\n",
      "  name: hello-pipeline\n",
      "root:\n",
      "  dag:\n",
      "    outputs:\n",
      "      parameters:\n",
      "        Output:\n",
      "          valueFromParameter:\n",
      "            outputParameterKey: Output\n",
      "            producerSubtask: how-are-you\n",
      "    tasks:\n",
      "      how-are-you:\n",
      "        cachingOptions:\n",
      "          enableCache: true\n",
      "        componentRef:\n",
      "          name: comp-how-are-you\n",
      "        dependentTasks:\n",
      "        - say-hello\n",
      "        inputs:\n",
      "          parameters:\n",
      "            hello_text:\n",
      "              taskOutputParameter:\n",
      "                outputParameterKey: Output\n",
      "                producerTask: say-hello\n",
      "        taskInfo:\n",
      "          name: how-are-you\n",
      "      say-hello:\n",
      "        cachingOptions:\n",
      "          enableCache: true\n",
      "        componentRef:\n",
      "          name: comp-say-hello\n",
      "        inputs:\n",
      "          parameters:\n",
      "            name:\n",
      "              componentInputParameter: recipient\n",
      "        taskInfo:\n",
      "          name: say-hello\n",
      "  inputDefinitions:\n",
      "    parameters:\n",
      "      recipient:\n",
      "        parameterType: STRING\n",
      "  outputDefinitions:\n",
      "    parameters:\n",
      "      Output:\n",
      "        parameterType: STRING\n",
      "schemaVersion: 2.1.0\n",
      "sdkVersion: kfp-2.4.0\n"
     ]
    }
   ],
   "source": [
    "!type pipeline.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4376f29d-d518-4fd7-9b2a-afc14774ea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT = gs://krishna-dotted-music-460617-k2-vertex-pipeline-bucket/pipeline_root\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"dotted-music-460617-k2\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "BUCKET_NAME = \"krishna-dotted-music-460617-k2-vertex-pipeline-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root\"\n",
    "print(\"PIPELINE_ROOT =\", PIPELINE_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41fa609b-0e33-42b9-adbf-e014a14eb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021b9eec-71c0-453f-963f-1de17ac33d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/hello-pipeline-20251217110702?project=864952758295\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702 current state:\n",
      "3\n",
      "PipelineJob run completed. Resource name: projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702\n",
      "RESOURCE NAME: projects/864952758295/locations/us-central1/pipelineJobs/hello-pipeline-20251217110702\n",
      "STATE: 4\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"hello-pipeline-run\",\n",
    "    template_path=\"pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"recipient\": \"Vamshi\"},\n",
    ")\n",
    "\n",
    "job.run(sync=True)  # IMPORTANT: sync=True for debugging\n",
    "print(\"RESOURCE NAME:\", job.resource_name)\n",
    "print(\"STATE:\", job.state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b142be-31eb-496d-89e3-a5110582d589",
   "metadata": {},
   "source": [
    "### these are the same \n",
    "### jsonl files from the previous lab\n",
    "\n",
    "### time stamps have been removed so that \n",
    "### the files are consistent for all learners\n",
    "TRAINING_DATA_URI = \"./tune_data_stack_overflow_python_qa.jsonl\" \n",
    "EVAUATION_DATA_URI = \"./tune_eval_data_stack_overflow_python_qa.jsonl\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f738b-45a0-43fd-aefa-e93bdfd8e29a",
   "metadata": {},
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "from utils import authenticate\n",
    "\n",
    "credentials, PROJECT_ID = authenticate()\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, credentials=credentials)\n",
    "\n",
    "BUCKET = \"krishna-dotted-music-460617-k2-vertex-pipeline-bucket\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET}/pipeline-root\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95608d-739e-459e-a223-faac3a3b4b19",
   "metadata": {},
   "source": [
    "TRAINING_DATA_URI = f\"gs://{BUCKET}/datasets/tune_data_stack_overflow_python_qa.jsonl\"\n",
    "EVALUATION_DATA_URI = f\"gs://{BUCKET}/datasets/tune_eval_data_stack_overflow_python_qa.jsonl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3c568-6906-4025-9efd-c5923ee0b422",
   "metadata": {},
   "source": [
    "import datetime\n",
    "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME = f\"deep-learning-ai-model-{date}\"\n",
    "\n",
    "TRAINING_STEPS = 200\n",
    "EVALUATION_INTERVAL = 20\n",
    "\n",
    "pipeline_arguments = {\n",
    "    \"model_display_name\": MODEL_NAME,\n",
    "    \"location\": REGION,\n",
    "    \"large_model_reference\": \"text-bison@001\",\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"train_steps\": TRAINING_STEPS,\n",
    "    \"dataset_uri\": TRAINING_DATA_URI,\n",
    "    \"evaluation_interval\": EVALUATION_INTERVAL,\n",
    "    \"evaluation_data_uri\": EVALUATION_DATA_URI,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4200014-3beb-466e-b445-06eefc7b0e82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "template_path = \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0\"\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"tune-llm-pipeline-{date}\",\n",
    "    template_path=template_path,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values=pipeline_arguments,\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit()\n",
    "print(\"Submitted:\", job.resource_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074d61c-3fb5-430d-9cc2-7df170064b3c",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ö†Ô∏è NOTE: Course Code Above is DEPRECATED\n",
    "\n",
    "The course used `text-bison@001` which Google deprecated in 2024.\n",
    "\n",
    "Below is the **updated code using Gemini** (the current model).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc591e61-ecb5-4fe3-a8e1-e2fe61bf7484",
   "metadata": {},
   "source": [
    "!pip install --upgrade google-cloud-aiplatform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd7ba9b-79d3-42c8-a9d9-6e4b995901c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ============================================================================\n",
    "# Gemini Fine-Tuning - Using v1beta1 API\n",
    "# ============================================================================\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import datetime\n",
    "\n",
    "# Configuration\n",
    "PROJECT_ID = \"dotted-music-460617-k2\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET = \"krishna-dotted-music-460617-k2-vertex-pipeline-bucket\"\n",
    "\n",
    "# Initialize\n",
    "print(\"üöÄ Initializing Vertex AI...\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Data paths\n",
    "TRAIN_GCS = f\"gs://{BUCKET}/datasets/tune_data_stack_overflow_python_qa.jsonl\"\n",
    "EVAL_GCS = f\"gs://{BUCKET}/datasets/tune_eval_stack_overflow_python_qa.jsonl\"\n",
    "\n",
    "# Model name\n",
    "date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_NAME = f\"stackoverflow-gemini-{date}\"\n",
    "\n",
    "print(f\"üìù Model Name: {MODEL_NAME}\")\n",
    "print(f\"üìÇ Training Data: {TRAIN_GCS}\")\n",
    "print(f\"üìÇ Validation Data: {EVAL_GCS}\")\n",
    "\n",
    "# Use v1beta1 (not v1)\n",
    "print(\"\\n‚è≥ Starting Gemini fine-tuning...\")\n",
    "\n",
    "from google.cloud.aiplatform_v1beta1 import GenAiTuningServiceClient  # ‚Üê Changed to v1beta1\n",
    "from google.cloud.aiplatform_v1beta1.types import SupervisedTuningSpec, TuningJob as TuningJobProto\n",
    "\n",
    "# Get the client\n",
    "client = GenAiTuningServiceClient(\n",
    "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    ")\n",
    "\n",
    "# Create tuning job\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "tuning_job = TuningJobProto(\n",
    "    base_model=\"gemini-1.5-flash-002\",\n",
    "    supervised_tuning_spec=SupervisedTuningSpec(\n",
    "        training_dataset_uri=TRAIN_GCS,\n",
    "        validation_dataset_uri=EVAL_GCS,\n",
    "        hyper_parameters=SupervisedTuningSpec.HyperParameters(\n",
    "            epoch_count=3,\n",
    "            learning_rate_multiplier=1.0,\n",
    "        )\n",
    "    ),\n",
    "    tuned_model_display_name=MODEL_NAME,\n",
    ")\n",
    "\n",
    "print(\"   Submitting job to Google Cloud...\")\n",
    "response = client.create_tuning_job(parent=parent, tuning_job=tuning_job)\n",
    "\n",
    "print(\"\\n‚úÖ SUCCESS! Training job submitted\")\n",
    "print(f\"\\nüìç Job Name:\")\n",
    "print(f\"   {response.name}\")\n",
    "\n",
    "print(f\"\\nüåê Monitor Progress:\")\n",
    "print(f\"   https://console.cloud.google.com/vertex-ai/generative/language/tuning/train?project={PROJECT_ID}\")\n",
    "\n",
    "# Save job name\n",
    "with open('tuning_job_name.txt', 'w') as f:\n",
    "    f.write(response.name)\n",
    "    \n",
    "print(\"\\n‚úì Job info saved to: tuning_job_name.txt\")\n",
    "print(\"\\n‚è≥ Training will take 20-40 minutes\")\n",
    "print(\"üí° You can close this notebook - training runs in the cloud!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61a98661-df3c-45a6-8a0a-5f26b55699c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PART 1: CHECKING v1beta1 MODULE\n",
      "======================================================================\n",
      "\n",
      "Total items in v1beta1: 697\n",
      "\n",
      "Tuning-related items found: 8\n",
      "  ‚úì CancelHyperparameterTuningJobRequest\n",
      "  ‚úì CreateHyperparameterTuningJobRequest\n",
      "  ‚úì DeleteHyperparameterTuningJobRequest\n",
      "  ‚úì GetHyperparameterTuningJobRequest\n",
      "  ‚úì HyperparameterTuningJob\n",
      "  ‚úì ListHyperparameterTuningJobsRequest\n",
      "  ‚úì ListHyperparameterTuningJobsResponse\n",
      "  ‚úì ManualBatchTuningParameters\n",
      "\n",
      "======================================================================\n",
      "PART 2: CHECKING SERVICES MODULE\n",
      "======================================================================\n",
      "\n",
      "Total services: 34\n",
      "\n",
      "Tuning-related services found: 1\n",
      "  ‚úì gen_ai_tuning_service\n",
      "    ‚Üí Can import: <module 'google.cloud.aiplatform_v1beta1.services.gen_ai_tuning_service' from 'C:\\\\projects\\\\vertex_ai_course\\\\.venv\\\\Lib\\\\site-packages\\\\google\\\\cloud\\\\aiplatform_v1beta1\\\\services\\\\gen_ai_tuning_service\\\\__init__.py'>\n",
      "\n",
      "======================================================================\n",
      "PART 3: CHECKING TYPES MODULE\n",
      "======================================================================\n",
      "\n",
      "Tuning-related types found: 12\n",
      "  ‚úì CancelHyperparameterTuningJobRequest\n",
      "  ‚úì CreateHyperparameterTuningJobRequest\n",
      "  ‚úì DeleteHyperparameterTuningJobRequest\n",
      "  ‚úì GetHyperparameterTuningJobRequest\n",
      "  ‚úì HyperparameterTuningJob\n",
      "  ‚úì ListHyperparameterTuningJobsRequest\n",
      "  ‚úì ListHyperparameterTuningJobsResponse\n",
      "  ‚úì ManualBatchTuningParameters\n",
      "  ‚úì genai_tuning_service\n",
      "  ‚úì hyperparameter_tuning_job\n",
      "\n",
      "======================================================================\n",
      "PART 4: CHECK PACKAGE VERSION INFO\n",
      "======================================================================\n",
      "\n",
      "Package version: 1.39.0\n",
      "Package location: C:\\projects\\vertex_ai_course\\.venv\\Lib\\site-packages\\google\\cloud\\aiplatform\\__init__.py\n",
      "\n",
      "======================================================================\n",
      "DIAGNOSTIC COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: Find the Correct Import Path\n",
    "# ============================================================================\n",
    "\n",
    "import google.cloud.aiplatform_v1beta1 as v1beta1\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: CHECKING v1beta1 MODULE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check what's in the main module\n",
    "available = dir(v1beta1)\n",
    "print(f\"\\nTotal items in v1beta1: {len(available)}\")\n",
    "\n",
    "# Look for anything tuning-related\n",
    "tuning_related = [item for item in available if 'tuning' in item.lower() or 'genai' in item.lower()]\n",
    "print(f\"\\nTuning-related items found: {len(tuning_related)}\")\n",
    "for item in tuning_related:\n",
    "    print(f\"  ‚úì {item}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 2: CHECKING SERVICES MODULE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from google.cloud.aiplatform_v1beta1 import services\n",
    "    services_list = dir(services)\n",
    "    print(f\"\\nTotal services: {len(services_list)}\")\n",
    "    \n",
    "    # Filter for tuning\n",
    "    tuning_services = [s for s in services_list if 'tuning' in s.lower() or 'genai' in s.lower() or 'gen_ai' in s.lower()]\n",
    "    print(f\"\\nTuning-related services found: {len(tuning_services)}\")\n",
    "    for service in tuning_services:\n",
    "        print(f\"  ‚úì {service}\")\n",
    "        \n",
    "        # Try to import it\n",
    "        try:\n",
    "            module = getattr(services, service)\n",
    "            print(f\"    ‚Üí Can import: {module}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚Üí Import error: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3: CHECKING TYPES MODULE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from google.cloud.aiplatform_v1beta1 import types\n",
    "    types_list = dir(types)\n",
    "    \n",
    "    tuning_types = [t for t in types_list if 'tuning' in t.lower() or 'supervised' in t.lower()]\n",
    "    print(f\"\\nTuning-related types found: {len(tuning_types)}\")\n",
    "    for t in tuning_types[:10]:  # Show first 10\n",
    "        print(f\"  ‚úì {t}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 4: CHECK PACKAGE VERSION INFO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import google.cloud.aiplatform\n",
    "print(f\"\\nPackage version: {google.cloud.aiplatform.__version__}\")\n",
    "print(f\"Package location: {google.cloud.aiplatform.__file__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a4462-6eef-4567-a21b-cff57c1114a3",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# Find Available Models for Tuning\n",
    "# ============================================================================\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1.services.gen_ai_tuning_service import GenAiTuningServiceClient\n",
    "\n",
    "PROJECT_ID = \"dotted-music-460617-k2\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Initialize\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Create client\n",
    "client = GenAiTuningServiceClient(\n",
    "    client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECKING AVAILABLE BASE MODELS FOR TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Try to list tuning jobs to see what models are being used\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "try:\n",
    "    # List existing tuning jobs to see what models are used\n",
    "    request = {\"parent\": parent}\n",
    "    page_result = client.list_tuning_jobs(request=request)\n",
    "    \n",
    "    print(\"\\nExisting tuning jobs in your project:\")\n",
    "    for response in page_result:\n",
    "        print(f\"  Model: {response.base_model}\")\n",
    "        print(f\"  Name: {response.name}\")\n",
    "        print(f\"  State: {response.state}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"No existing jobs or error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMMON MODEL NAMES FOR TUNING:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Try these model names:\n",
    "1. text-bison@002 (newer Bison, might still work)\n",
    "2. gemini-1.0-pro-002\n",
    "3. gemini-1.5-pro-002  \n",
    "4. gemini-1.5-flash-001\n",
    "5. publishers/google/models/gemini-1.5-flash-002\n",
    "\n",
    "Let's test which one works...\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6a80df-c80c-4a09-964b-63a78a848ada",
   "metadata": {},
   "source": [
    "# Test which model name works\n",
    "test_models = [\n",
    "    \"text-bison@002\",\n",
    "    \"gemini-1.0-pro-002\",\n",
    "    \"gemini-1.5-pro-002\",\n",
    "    \"gemini-1.5-flash-001\",\n",
    "    \"publishers/google/models/gemini-1.5-flash-002\",\n",
    "    \"gemini-1.5-flash-002\",\n",
    "]\n",
    "\n",
    "for model_name in test_models:\n",
    "    print(f\"\\nTesting: {model_name}\")\n",
    "    try:\n",
    "        from google.cloud.aiplatform_v1beta1.types import TuningJob, SupervisedTuningSpec\n",
    "        \n",
    "        tuning_job = TuningJob(\n",
    "            base_model=model_name,\n",
    "            supervised_tuning_spec=SupervisedTuningSpec(\n",
    "                training_dataset_uri=TRAIN_GCS,\n",
    "                validation_dataset_uri=EVAL_GCS,\n",
    "                hyper_parameters={\"epoch_count\": 3, \"learning_rate_multiplier\": 1.0}\n",
    "            ),\n",
    "            tuned_model_display_name=f\"test-{model_name.replace('/', '-')}\",\n",
    "        )\n",
    "        \n",
    "        # Try to submit (will validate model name)\n",
    "        response = client.create_tuning_job(parent=parent, tuning_job=tuning_job)\n",
    "        print(f\"  ‚úÖ SUCCESS! {model_name} works!\")\n",
    "        print(f\"  Job: {response.name}\")\n",
    "        break  # Stop on first success\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        if \"not supported\" in error_msg:\n",
    "            print(f\"  ‚ùå Not supported\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Error: {error_msg[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec0a7b-0fd8-40a8-82a9-de73fc638c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vertex-ai-env)",
   "language": "python",
   "name": "vertex-ai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
